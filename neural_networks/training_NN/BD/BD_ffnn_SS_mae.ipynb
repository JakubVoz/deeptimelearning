{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "# following versions need to be installed\n",
    "pkg_resources.require('ete3==3.1.1', 'pandas==0.23.4', 'numpy==1.18.5', 'scipy==1.1.0', 'scikit-learn==0.19.1',\n",
    "                      'tensorflow==1.13.1', 'joblib==0.13.2', 'h5py==2.10.0', 'Keras==2.3.1', 'matplotlib==3.1.3')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.layers import Activation, Dense\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutoff=1000000 #all\n",
    "\n",
    "#Reading input data, features and parameters\n",
    "param_train = pd.read_csv('../../../results_eBDM_vs3_200_500_tips/csv_for_train.csv')\n",
    "param_test = pd.read_csv('../../../results_eBDM_vs3_200_500_tips/csv_parameters_for_test.csv')\n",
    "param_train['diversification_rate'] = param_train['birth_rate'] - param_train['extinction_rate']\n",
    "param_test['diversification_rate'] = param_test['birth_rate'] - param_test['extinction_rate']\n",
    "\n",
    "\n",
    "#Reshape parameters: drop index column\n",
    "param_train = param_train.drop(param_train.columns[0],1)\n",
    "param_test = param_test.drop(param_test.columns[0], 1)\n",
    "\n",
    "encoding_train = pd.read_csv('../../../results_eBDM_vs3_200_500_tips/TARGET_MEDIAN_sumstats_ecology_vs3_forced_200_500_tips_train_01_1/sumstats_data.csv', sep=\"\\t\", header=None)\n",
    "encoding_test = pd.read_csv('../../../results_eBDM_vs3_200_500_tips/TARGET_MEDIAN_sumstats_ecology_vs3_forced_200_500_tips_test_01_1/sumstats_data.csv', sep=\"\\t\", header=None)\n",
    "\n",
    "chemin = 'sumstats/all_samp_input/'\n",
    "expname='_ffnn_ss_mae'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correctly reshape parameters (rescaling) and encodings (rescale factor):\n",
    "\n",
    "### TRAINING SET: PARAMETER VALUES\n",
    "# drop rescaling factor and add it to param tables\n",
    "param_train['scaling_factor'] = encoding_train.iloc[:,-1]\n",
    "encoding_train = encoding_train.drop(encoding_train.columns[-1], axis =1)\n",
    "# rescale target values corresponding to the rescale factor\n",
    "param_train['birth_rate_resc'] = param_train['birth_rate']*param_train['scaling_factor']\n",
    "param_train['extinction_rate_resc'] = param_train['extinction_rate']*param_train['scaling_factor']\n",
    "param_train['diversification_rate_resc'] = param_train['diversification_rate']*param_train['scaling_factor']\n",
    "\n",
    "### TESTING SET: PARAMETER VALUES\n",
    "param_test['scaling_factor'] = encoding_test.iloc[:,-1]\n",
    "encoding_test = encoding_test.drop(encoding_test.columns[-1], axis =1)\n",
    "# rescale target values corresponding to the rescale factor\n",
    "param_test['birth_rate_resc'] = param_test['birth_rate']*param_test['scaling_factor']\n",
    "param_test['extinction_rate_resc'] = param_test['extinction_rate']*param_test['scaling_factor']\n",
    "param_test['diversification_rate_resc'] = param_test['diversification_rate']*param_test['scaling_factor']\n",
    "\n",
    "\n",
    "#Reshape features\n",
    "encoding_train = encoding_train.drop(encoding_train.columns[0], axis =1)\n",
    "encoding_test = encoding_test.drop(encoding_test.columns[0], axis =1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "print(encoding_test.shape)\n",
    "print(param_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "param_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choice of the parameters to predict\n",
    "target_1 = \"turnover_rate\"\n",
    "target_2 = \"birth_rate_resc\"\n",
    "target_3 = \"extinction_rate_resc\"\n",
    "target_4 = \"diversification_rate_resc\"\n",
    "\n",
    "targets = pd.DataFrame(param_train[[target_1, target_2, target_3, target_4]])\n",
    "targets_test = pd.DataFrame(param_test[[target_1, target_2, target_3, target_4]])\n",
    "\n",
    "features = encoding_train\n",
    "features_test = encoding_test\n",
    "\n",
    "# how large is the validation set\n",
    "valid_set_nb = 10000\n",
    "valid_frac = valid_set_nb/features.shape[0]\n",
    "train_size_frac = (features.shape[0]-valid_set_nb)/features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the known sampling fraction into the representation (both train and test sets)\n",
    "add_target = \"sampling_frac\"\n",
    "added_targets = pd.DataFrame(param_train[add_target])\n",
    "features['399'] = added_targets\n",
    "\n",
    "add_target_2 = \"sampling_frac\"\n",
    "added_targets_2 = pd.DataFrame(param_test[add_target])\n",
    "features_test['399'] = added_targets_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization of the input features with a standard scaler\n",
    "scale = StandardScaler()\n",
    "features = scale.fit_transform(features)\n",
    "features_test = scale.fit_transform(features_test)\n",
    "\n",
    "X = features\n",
    "Y = targets\n",
    "\n",
    "Y_test = targets_test\n",
    "X_test = features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the Network Model: model definition\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(64, input_dim=98, activation='elu'))\n",
    "    keras.layers.Dropout(0.5)\n",
    "    model.add(Dense(32, activation='elu'))\n",
    "    keras.layers.Dropout(0.5)\n",
    "    model.add(Dense(16, activation='elu'))\n",
    "    keras.layers.Dropout(0.5)\n",
    "    model.add(Dense(8, activation='elu'))\n",
    "    keras.layers.Dropout(0.5)\n",
    "    model.add(Dense(4, activation='linear'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Building of the model\n",
    "\n",
    "from keras import losses\n",
    "\n",
    "# model creation\n",
    "model = build_model()\n",
    "\n",
    "estimator = model\n",
    "\n",
    "#Adam optimizer, loss measure: mean absolute error, metrics measured: MAPE\n",
    "estimator.compile(loss='mae', optimizer = 'Adam', metrics=[losses.mean_absolute_percentage_error])\n",
    "\n",
    "#early stopping to avoid overfitting\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "#display training progress for each completed epoch.\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self,epoch,logs):\n",
    "    if epoch % 100 == 0: print('')\n",
    "    print('.', end='')\n",
    "\n",
    "# maximum number of EPOCHS, ie full training cycles on the whole training dataset (how many times we see the same training set)\n",
    "EPOCHS = 10000\n",
    "\n",
    "#Training of the Network, with an independent validation set\n",
    "history = estimator.fit(X, Y, verbose = 1, epochs=EPOCHS, validation_split=valid_frac, batch_size=8000, callbacks=[early_stop, PrintDot()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#import statsmodel.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot test vs predicted\n",
    "# predict values for the test set\n",
    "predicted_test = pd.DataFrame(estimator.predict(X_test))\n",
    "predicted_test.columns = Y_test.columns # rename correctly the columns\n",
    "predicted_test.index = Y_test.index # rename indexes for correspondence\n",
    "\n",
    "elts = []\n",
    "\n",
    "# just for subsetting columns more automatically + naming output plots\n",
    "for elt in Y_test.columns:\n",
    "    elts.append(elt)\n",
    "\n",
    "for elt in elts:\n",
    "    sub_df = pd.DataFrame({'predicted_minus_target_' + elt: predicted_test[elt] - Y_test[elt], 'target_'+elt: Y_test[elt], 'predicted_'+elt: predicted_test[elt]})\n",
    "    if elt == elts[0]:\n",
    "        df = sub_df\n",
    "    else:\n",
    "        sub_df.index = df.index\n",
    "        df = pd.concat([df, sub_df], axis=1)\n",
    "\n",
    "# fast plotting for analysis (with seaborn):\n",
    "def target_vs_predicted2(target_name, predicted_name, param_name, file_name_beg) : \n",
    "    sns.set_style('white')\n",
    "    sns.set_context('talk')\n",
    "    sns.regplot(x=target_name, y=predicted_name, data=df, ci=95, n_boot=500, \n",
    "                scatter_kws={'s':0.1, 'color':'grey'}, line_kws={ 'color':'green', 'linewidth':2})\n",
    "    plt.title(param_name + ': target vs predicted test dataset')\n",
    "    plt.xlabel('target')\n",
    "    plt.ylabel('predicted')\n",
    "    innerlimit = min(df[target_name])\n",
    "    \n",
    "    outerlimit = max(df[target_name])\n",
    "    plt.plot([innerlimit, outerlimit], [innerlimit, outerlimit], linewidth=2, color='red')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "    \n",
    "for elt in elts:\n",
    "    target_vs_predicted2('target_'+elt, 'predicted_'+elt, elt, file_name_beg=elt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# table with statistics on errors\n",
    "errors_index = elts\n",
    "errors_columns = ['MAE', 'RMSE', 'RME']\n",
    "errors = pd.DataFrame(index=errors_index, columns=errors_columns)\n",
    "\n",
    "def get_mae_rmse(name_var):\n",
    "    predicted_vals = df['predicted_' + name_var]\n",
    "    target_vals = df['target_' + name_var]\n",
    "    diffs_abs = abs(target_vals - predicted_vals)\n",
    "    diffs_rel = diffs_abs/target_vals\n",
    "    diffs_abs_squared = diffs_abs**2\n",
    "    mae = np.sum(diffs_abs)/len(diffs_abs)\n",
    "    rmse = np.sqrt(sum(diffs_abs_squared)/len(diffs_abs_squared))\n",
    "    rme = np.sum(diffs_rel)/len(diffs_rel)\n",
    "    return mae, rmse, rme\n",
    "    \n",
    "\n",
    "for elt in errors_index:\n",
    "    errors.loc[elt] = np.array(get_mae_rmse(elt))\n",
    "\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###save the model, weights and scaler\n",
    "\n",
    "#save model and model weights + scaler\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "model_trial_1000 = model.to_json()\n",
    "with open('../../Model/' + chemin + 'model_all' + expname + '.json','w') as json_file:\n",
    "    json_file.write(model_trial_1000)\n",
    "\n",
    "model.save_weights('../../Model/' + chemin + 'model_all_weights' + expname +'.h5')\n",
    "print('model saved!')\n",
    "\n",
    "#save scaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "scale_filename = '../../Model/' + chemin + 'all_standardscaler' + expname + '.pkl'\n",
    "joblib.dump(scale, scale_filename)\n",
    "\n",
    "print('scale saved!')\n",
    "#load scaler\n",
    "#scale = joblib.load(scale_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####save the predicted and the target dataframes\n",
    "\n",
    "Y_test.to_csv('../../Data/' + chemin + expname + 'target_all.csv', header=True)\n",
    "\n",
    "predicted_test['scaling_factor'] = param_test['scaling_factor']\n",
    "\n",
    "predicted_test['birth_rate'] = predicted_test['birth_rate_resc']/predicted_test['scaling_factor']\n",
    "\n",
    "predicted_test['extinction_rate'] = predicted_test['extinction_rate_resc']/predicted_test['scaling_factor']\n",
    "\n",
    "predicted_test['diversification_rate'] = predicted_test['diversification_rate_resc']/predicted_test['scaling_factor']\n",
    "\n",
    "predicted_test.to_csv('../../Data/' + chemin + expname + 'predicted_all.csv', header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##for CI predict\n",
    "\n",
    "predicted_CI = pd.DataFrame(estimator.predict(features_CI))\n",
    "\n",
    "predicted_CI.columns = Y_test.columns\n",
    "\n",
    "predicted_CI.to_csv('../../Data/' + chemin + 'predicted_CI_all' + expname + '.csv', header=True)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
