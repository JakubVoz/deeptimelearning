{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "# following versions need to be installed\n",
    "pkg_resources.require('ete3==3.1.1', 'pandas==0.23.4', 'numpy==1.18.5', 'scipy==1.1.0', 'scikit-learn==0.19.1',\n",
    "                      'tensorflow==1.13.1', 'joblib==0.13.2', 'h5py==2.10.0', 'Keras==2.3.1', 'matplotlib==3.1.3')\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense\n",
    "\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########loading data#########\n",
    "# we load files with parameter values and files with tree representations (here full tree representations)\n",
    "\n",
    "cutoff=1000000 #all: 1M examples for training\n",
    "\n",
    "# loading parameters\n",
    "param_train = pd.read_csv('..', nrows=cutoff, header=None, sep='\\t', index_col=0, skiprows=1) # options: nrows: how many rows to load, sep: column separator, header: no header, index_col: the first column corresponds to indexes\n",
    "param_test = pd.read_csv('..', sep='\\t', header=None, index_col=0, skiprows=1)\n",
    "\n",
    "# for renaming columns in parameter files\n",
    "column_names = ['lambda1', 'lambda2', 'turnover', 'sampling_frac', 'tree_size', 'mu1', 'mu2', 'net_rate1', 'net_rate2', 'q01', 'q10', 'lambda2_ratio', 'q01_ratio']\n",
    "\n",
    "\n",
    "def rename_columns(df, names):\n",
    "    df = df.rename(columns={i: names[int(i)-1] for i in df.columns})\n",
    "    return df\n",
    "\n",
    "param_train = rename_columns(param_train, column_names)\n",
    "param_test = rename_columns(param_test, column_names)\n",
    "\n",
    "# loading tree encodings/representations\n",
    "# encoding has the following structure: 1 value of tree height, 500 values for tip states ('1' or '2')\n",
    "# 1 value for tree height and 500 values for internal node heights\n",
    "# + 2 values for nb of tips of each type (to be removed) and 1 value of rescaling (removed, but stocked for rescaling predicted values back to the original scale)\n",
    "encoding_train = pd.read_csv('../', sep=\"\\t\", header=None, nrows=cutoff, index_col=0)\n",
    "encoding_test = pd.read_csv('../', sep=\"\\t\", header=None, index_col=0)\n",
    "\n",
    "# make sure there is correspondance between indexes of dataframe with parameter values and encodings\n",
    "encoding_train.index = param_train.index\n",
    "encoding_test.index = param_test.index\n",
    "\n",
    "# part of the relative path for writing down the output files\n",
    "chemin = 'full_tree/all_samp_input/'\n",
    "\n",
    "# the suffix of output files\n",
    "expname='_1000000_longest_absolute_error'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "param_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "print(encoding_test.shape)\n",
    "print(param_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "encoding_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correctly reshape parameters (rescaling) and encodings (remove nb of each type + rescale factor):\n",
    "\n",
    "### TRAINING SET: PARAMETER VALUES\n",
    "# rescaling factor\n",
    "param_train['norm_factor'] = encoding_train[1005]\n",
    "# rescale target values according to scaling factor\n",
    "param_train['net_rat1_rescaled'] = param_train['net_rate1']*param_train['norm_factor']\n",
    "param_train['net_rat2_rescaled'] = param_train['net_rate2']*param_train['norm_factor']\n",
    "param_train['lambda1_rescaled'] = param_train['lambda1']*param_train['norm_factor']\n",
    "param_train['lambda2_rescaled'] = param_train['lambda2']*param_train['norm_factor']\n",
    "param_train['q01_rescaled'] = param_train['q01']*param_train['norm_factor']\n",
    "\n",
    "### TESTING SET: PARAMETER VALUES\n",
    "# rescaling factor\n",
    "param_test['norm_factor'] = encoding_test[1005]\n",
    "# rescale target values\n",
    "param_test['net_rat1_rescaled'] = param_test['net_rate1']*param_test['norm_factor']\n",
    "param_test['net_rat2_rescaled'] = param_test['net_rate2']*param_test['norm_factor']\n",
    "param_test['lambda1_rescaled'] = param_test['lambda1']*param_test['norm_factor']\n",
    "param_test['lambda2_rescaled'] = param_test['lambda2']*param_test['norm_factor']\n",
    "param_test['q01_rescaled'] = param_test['q01']*param_test['norm_factor']\n",
    "\n",
    "# remove irrelevant columns: count of each type of tip and normalization factor\n",
    "encoding_train.drop(columns=[1003, 1004, 1005], axis=1, inplace=True)\n",
    "encoding_test.drop(columns=[1003, 1004, 1005], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choice of the parameters to predict\n",
    "target_1 = \"turnover\"\n",
    "target_2 = \"lambda1_rescaled\"\n",
    "target_3 = \"lambda2_rescaled\"\n",
    "target_4 = \"q01_rescaled\"\n",
    "\n",
    "targets = pd.DataFrame(param_train[[target_1, target_2, target_3, target_4]])\n",
    "targets_test = pd.DataFrame(param_test[[target_1, target_2, target_3, target_4]])\n",
    "\n",
    "features = encoding_train\n",
    "features_test = encoding_test\n",
    "\n",
    "# how large is the validation set\n",
    "valid_set_nb = 10000\n",
    "valid_frac = valid_set_nb/features.shape[0]\n",
    "train_size_frac = (features.shape[0]-valid_set_nb)/features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the known sampling fraction as 3*2 matrix into the representation (both train and test sets)\n",
    "add_target = \"sampling_frac\"\n",
    "added_targets = pd.DataFrame(param_train[add_target])\n",
    "features['1003'] = added_targets\n",
    "features['1004'] = added_targets\n",
    "features['1005'] = added_targets\n",
    "features['1006'] = added_targets\n",
    "features['1007'] = added_targets\n",
    "features['1008'] = added_targets\n",
    "\n",
    "added_targets2 = pd.DataFrame(param_test[add_target])\n",
    "features_test['1003'] = added_targets2\n",
    "features_test['1004'] = added_targets2\n",
    "features_test['1005'] = added_targets2\n",
    "features_test['1006'] = added_targets2\n",
    "features_test['1007'] = added_targets2\n",
    "features_test['1008'] = added_targets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearranging the matrix (for each tre there is 1st vector (internal tips info) followed by 2nd vector (states at external tips))\n",
    "tips_coor = np.arange(0,501)\n",
    "samp = np.array([1002, 1003, 1004]) # this is corresponding to the sampling fraction\n",
    "tips_coor = np.append(tips_coor, samp)\n",
    "branches_coor = np.arange(501,1002)\n",
    "samp2 = np.array([1005, 1006, 1007]) # this is corresponding to the sampling fraction\n",
    "branches_coor = np.append(branches_coor, samp2)\n",
    "\n",
    "#type_coor = np.arange(459,689,1) \n",
    "\n",
    "\n",
    "order_corr = np.append(branches_coor, tips_coor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if in good order\n",
    "branches_coor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearranging (for full tree BiSSE)\n",
    "features = features.iloc[:, order_corr]\n",
    "features_test = features_test.iloc[:, order_corr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization of the input features: done for summary statistics\n",
    "\"\"\"\n",
    "scale = StandardScaler()\n",
    "features = scale.fit_transform(features)\n",
    "features_test = scale.transform(features_test)\n",
    "\"\"\"\n",
    "\n",
    "X = features\n",
    "Y = targets\n",
    "\n",
    "Y_test = targets_test\n",
    "X_test = features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the Network Model: model definition\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(keras.layers.Reshape((504, 2), input_shape=(X.shape[1],))) # the input of 1008 columns is reshaped into 504*2 (one column for tip states, the other for internal nodes distances + repeated value of tree height + 3*sampling fraction)\n",
    "    # convolutional part\n",
    "    model.add(Conv1D(filters = 50, kernel_size=(5), input_shape= (504, 2), activation='elu'))\n",
    "    model.add(Conv1D(filters = 50, kernel_size=(10), activation='elu'))\n",
    "    model.add(MaxPooling1D(10))\n",
    "    model.add(Conv1D(filters = 80, kernel_size=(10), activation='elu'))\n",
    "    # flattening the 2D 'feature maps' into 1D vector used in 'FFNN part'\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    # FFNN part\n",
    "    keras.layers.Dropout(0.5)\n",
    "    model.add(Dense(64, activation='elu'))\n",
    "    keras.layers.Dropout(0.5)\n",
    "    model.add(Dense(32, activation='elu'))\n",
    "    keras.layers.Dropout(0.5)\n",
    "    model.add(Dense(16, activation='elu'))\n",
    "    keras.layers.Dropout(0.5)\n",
    "    model.add(Dense(8, activation='elu'))\n",
    "    keras.layers.Dropout(0.5)\n",
    "    # output layer with 4 output neurons = nb of target parameters \n",
    "    model.add(Dense(4, activation='elu'))\n",
    "    # show the model structure\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Building of the model\n",
    "\n",
    "from keras import losses\n",
    "\n",
    "#model creation\n",
    "estimator = build_model()\n",
    "\n",
    "#Adam optimizer, loss measure: mean absolute error, metrics measured: MAPE\n",
    "estimator.compile(loss='mae', optimizer = 'Adam', metrics=[losses.mean_absolute_percentage_error])\n",
    "\n",
    "#early stopping to avoid overfitting\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "#display training progress for each completed epoch.\n",
    "class PrintD(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self,epoch,logs):\n",
    "    if epoch % 100 == 0: print('')\n",
    "    print('.', end='')\n",
    "\n",
    "# maximum number of EPOCHS, ie full training cycles on the whole training dataset (how many times we see the same training set)\n",
    "EPOCHS = 10000\n",
    "\n",
    "#Training of the Network, with an independent validation set\n",
    "history = estimator.fit(X, Y, verbose = 1, epochs=EPOCHS, validation_split=valid_frac, batch_size=8000, callbacks=[early_stop, PrintD()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#import statsmodel.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot test vs predicted\n",
    "# predict values for the test set\n",
    "predicted_test = pd.DataFrame(estimator.predict(X_test))\n",
    "predicted_test.columns = Y_test.columns # rename correctly the columns\n",
    "predicted_test.index = Y_test.index # rename indexes for correspondence\n",
    "\n",
    "elts = []\n",
    "\n",
    "# just for subsetting columns more automatically + naming output plots\n",
    "for elt in Y_test.columns:\n",
    "    elts.append(elt)\n",
    "\n",
    "for elt in elts:\n",
    "    sub_df = pd.DataFrame({'predicted_minus_target_' + elt: predicted_test[elt] - Y_test[elt], 'target_'+elt: Y_test[elt], 'predicted_'+elt: predicted_test[elt]})\n",
    "    if elt == elts[0]:\n",
    "        df = sub_df\n",
    "    else:\n",
    "        sub_df.index = df.index\n",
    "        df = pd.concat([df, sub_df], axis=1)\n",
    "\n",
    "# fast plotting for analysis (with seaborn):\n",
    "def target_vs_predicted(target_name, predicted_name, param_name, file_name_beg) : \n",
    "    sns.set_style('white')\n",
    "    sns.set_context('talk')\n",
    "    sns.regplot(x=target_name, y=predicted_name, data=df, ci=95, n_boot=500, \n",
    "                scatter_kws={'s':0.1, 'color':'grey'}, line_kws={ 'color':'green', 'linewidth':2})\n",
    "    plt.title(param_name + ': target vs predicted test dataset')\n",
    "    plt.xlabel('target')\n",
    "    plt.ylabel('predicted')\n",
    "    innerlimit = min(df[target_name])\n",
    "    \n",
    "    outerlimit = max(df[target_name])\n",
    "    plt.plot([innerlimit, outerlimit], [innerlimit, outerlimit], linewidth=2, color='red')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "    \n",
    "for elt in elts:\n",
    "    target_vs_predicted('target_'+elt, 'predicted_'+elt, elt, file_name_beg=elt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# table with statistics on errors\n",
    "errors_index = elts\n",
    "errors_columns = ['MAE', 'RMSE', 'RME']\n",
    "errors = pd.DataFrame(index=errors_index, columns=errors_columns)\n",
    "\n",
    "def get_mae_rmse(name_var):\n",
    "    predicted_vals = df['predicted_' + name_var]\n",
    "    target_vals = df['target_' + name_var]\n",
    "    diffs_abs = abs(target_vals - predicted_vals)\n",
    "    diffs_rel = diffs_abs/target_vals\n",
    "    diffs_abs_squared = diffs_abs**2\n",
    "    mae = np.sum(diffs_abs)/len(diffs_abs)\n",
    "    rmse = np.sqrt(sum(diffs_abs_squared)/len(diffs_abs_squared))\n",
    "    rme = np.sum(diffs_rel)/len(diffs_rel)\n",
    "    return mae, rmse, rme\n",
    "    \n",
    "\n",
    "#errors.loc['R_nought'] = np.array(get_mae_rmse('R_nought'))\n",
    "for elt in errors_index:\n",
    "    errors.loc[elt] = np.array(get_mae_rmse(elt))\n",
    "\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print differences between predicted and target as function of target: showing structural bias\n",
    "\n",
    "def predicted_minus_target_vs_target(pr_m_tar_name, target_name, param_name, file_name_beg) : \n",
    "    sns.set_style('white')\n",
    "    sns.set_context('talk')\n",
    "    sns.regplot(x=target_name, y=pr_m_tar_name, data=df, ci=95, n_boot=500, \n",
    "                scatter_kws={'s':0.1, 'color':'grey'}, line_kws={ 'color':'green', 'linewidth':2})\n",
    "    plt.title(param_name + ': target vs (target-predicted) test dataset')\n",
    "    plt.xlabel('target')\n",
    "    plt.ylabel('target - predicted')\n",
    "    innerlimit = min(df[target_name])\n",
    "    \n",
    "    outerlimit = max(df[target_name])\n",
    "    \n",
    "    plt.plot([innerlimit, outerlimit], [0, 0], linewidth=2, color='red')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "for elt in elts:\n",
    "    predicted_minus_target_vs_target('predicted_minus_target_'+elt, 'target_'+elt, elt, file_name_beg=elt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure of correlation: predicted vs target\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "pearson_cors = []\n",
    "\n",
    "for elt in elts:\n",
    "    pearson_cors.append(pearsonr(Y_test[elt], predicted_test[elt])[0])\n",
    "\n",
    "print(\"Global pearson correlation between predicted and effective parameter: \", Y_test.columns, pearson_cors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###save the model, weights (and scaler for sumstats only)\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# save model\n",
    "model_trial_1000 = model.to_json()\n",
    "with open('../Model/' + chemin + 'model_all' + expname + '.json','w') as json_file:\n",
    "    json_file.write(model_trial_1000)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('../Model/' + chemin + 'model_all_weights' + expname +'.h5')\n",
    "print('model saved!')\n",
    "\n",
    "'''\n",
    "#load the model\n",
    "json_file = open('../Model/' + chemin + 'model_all' + expname + '.json', 'r')\n",
    "loaded_file = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_file)\n",
    "#load weights\n",
    "loaded_model.load_weights(../Model/' + chemin + 'model_all_weights' + expname +'.h5'5)\n",
    "print('model loaded!')\n",
    "\n",
    "'''\n",
    "\n",
    "#save scaler when there is one (FFNN-SS)\n",
    "\"\"\"\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "scale_filename = '../../Model/' + chemin + 'all_standardscaler' + expname + '.pkl'\n",
    "joblib.dump(scale, scale_filename)\n",
    "\n",
    "print('scale saved!')\n",
    "#load scaler:\n",
    "#scale = joblib.load(scale_filename)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####save the predicted and the target dataframes\n",
    "\n",
    "Y_test.to_csv('../Data/' + chemin + expname + 'target_all.csv', header=True)\n",
    "\n",
    "predicted_test.to_csv('../Data/' + chemin + expname + 'predicted_all.csv', header=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
